We thank the reviewers of our paper for providing us with insightful comments
and suggestions to help improve our paper. We agree with most of the points
raised by the reviewers have attempted to address all of their concerns. This
document describes how we have addressed their concerns.

===============================================================================

Comments by Reviewer #1
-----------------------

** CONCERN #1: "The paper never substantiates its claim that Gibraltar's
   automated generation of kernel invariants produces a better description of
   the kernel than previous manual descriptions of the kernel, for use in
   detecting root kits."

++ RESPONSE: We recognize the concern raised by this reviewer. For example, we
   acknowledge that a well-crafted manual invariant may subsume hundreds of
   automatically-inferred invariants. However, we appreciate that the prose in
   our paper might have given this reviewer (and indeed, other readers) the
   illusion that we were claiming that automatically inferred invariants are
   better than those written by a kernel expert. Evaluating the quality of the
   automatically inferred invariants, and comparing them against those written
   by an expert is beyond the scope of the current paper.

   To address this conern, we have rewritten the abstract, portions of the
   introduction and the conclusions with a reduced claim. We decided to follow
   this reviewer's suggestion that we state that "our work demonstrates that
   automated generation of invariants is feasible, but the merits of the
   resulting invariant set wrt to manually expert generated invariant sets is
   unknown?" In particular, we refer the reviewer to the paragraph in the
   Introduction just preceding the list of contributions, where we explicitly
   address this concern using the reviewer's suggestion.

-----

** CONCERN #2: "Useful security research needs to also consider, address, and
   investigate the response of a human adversary to the proposed solution. What
   if an attacker uses a private copy of the proposed Gibraltar solution to
   find a rootkit that does not use any of the invariants it generates? Is
   there any analysis of how difficult this would be?"

++ RESPONSE: It is definitely possible for an informed attacker to write
   rootkits that can bypass Gibraltar. For example, the attacker could
   carefully craft a rootkit can satisfies all the inferred invariants but
   still maliciously attacks kernel data structures. 

   Nevertheless, we believe that Gibraltar raises the bar for an attacker, as
   is evidenced by the fact that it could detect all 23 rootkits that we tested
   it against. We have not yet done a formal analysis of the difficulty of
   writing such a rootkit, and mention this as a point for future
   investigation. See the "Conclusions and Future Work" section for a
   discussion of this point.

-----

** CONCERN #3: "Does the proposed Gibraltar solution always generate the same
   set of invariants, for a given platform configuration? If we take 32 identical
   platforms and run each of them with different human users, for training, do we
   get the same set of invariants? If the training set is always the same, how do
   we know it matches the actual user input set? How do we know that the set of
   invariants generated by Gibraltar is good?"

++ RESPONSE: For a given platform configuration and a given inference workload,
   Gibraltar would produce the same set of persistent invariants. We were unclear
   about the reviewer's comment "If the training set is always the same, how do
   we know it matches the actual user input set?" In particular, the training set
   is a set of tasks chosen, say, by the OS vendor, to exercise different control
   flow paths in the kernel. For our purposes, we chose the LMBench workload for
   inference, but an OS vendor would obviously have access to more "typical user"
   workloads over which to train the system. We currently do not have a good
   answer to the last question raised by the reviewer: i.e., how do we know 
   whether the invariants generated by Gibraltar are good? We propose to
   investigate this in future work, and mention this in the paper.

-----

** CONCERN #4: "What if some users update their operating system software, say
   by a security patch. Do we now have to retrain the system?"

++ RESPONSE: In our current prototype, we have to retrain the system. However,
   a future enhancement may make it possible to incrementally update invariants
   by performing a change-impact analysis to study how the patch affects the
   data structure. This has been addressed in the future work section.

-----

** CONCERN #5: "Another issue with this research that has not been addressed is
   attacks that do not modify the kernel, but do not use it any more. That is,
   the program counter is actually executing a virtual kernel located in high
   memory and using trampolines (or a similar technique), so no invariants are
   changed.  It is also possible to have bad things happening, but not on the
   front side bus, i.e. malware in the IOAPIC."

++ RESPONSE: We definitely acknowledge that there are rootkit attacks that
   cannot be detected by Gibraltar. Our system can detect rootkits that violate
   kernel data structure invariants, but the kind of rootkits pointed out by
   the reviewer can definitely bypass our system. So can rootkits that employ
   hypervisor-based techniques (e.g., BluePill). Our paper also does not
   make any claims about Gibraltar's ability to detect such rootkits.

-----

** CONCERN #6: "The paper does not give details of how intelligent peripherals
   can be blocked, but merely references the ever cryptic J. Rutkowska. What
   technique or trick causes a problem, according to Rutkowska? .... Tell us 
   what the web page said."

++ RESPONSE: We have briefly summarized Rutkowska's attack in the paper.

-----
   
** CONCERN #7: "A large number of web page references, for a journal paper,
   perhaps too many?  None of the web page references have a date of reference
   on them. At a minimum, each reference to a web page should have the date the
   page was referenced."

++ RESPONSE: We have fixed this issue by adding the date that we last visited
   all of these URLs (please see the end of the references section). Where
   applicable, we also refer ro named content that was downloaded and
   referenced. We have also paid careful attention to formatting issues in
   the references in this revision of our article.


===============================================================================


Comments by Reviewer #2
-----------------------
We particularly thank reviewer #2 for deep and insightful comments on the
editorial quality of Sections 2 and 4.3, and our experimental methodology. 
We have done our best to incorporate his/her suggestions.

This reviewer had five high-level concerns about the technical content of 
our paper:
CONCERN #1: False positive rate.
CONCERN #2: Related work.
CONCERN #3: Inconsistent memory snapshots.
CONCERN #4: Invariant generator enhancements (Section 4.3).
CONCERN #5: Experimental methodology.
We address each of these concerns separately below.

-----

** CONCERN #1: "From the first mention of the false positive rate on page 2, I
   wondered what the denominator of this fraction was ... Only in section 5.4
   is the measurement defined, and it is bogus: the false positive computation
   is incorrect."

++ RESPONSE: A very valid point. Our objective was to determine how many
   invariants were spuriously violated when we executed the kernel on a benign
   input. We no longer use the term "false positives", but instead use the more
   precise term of "spurious alerts", and actually present the number of
   invariants that were violated during our test run. We present both the
   number of alerts that we observed, as well as the number (and percentage)
   of persistent invariants that were spuriously violated.

   We tested our system on just one benign workload that spanned 42 minutes,
   during which we observed alerts. During this execution, there were 85 alerts
   for persistent invariants. The reviewer is therefore correct in that there
   were no benign executions with 0 alerts. We also report the number with
   transient invariants, though, as suggested by this reviewer (c.f. CONCERN
   #4), we have refocused our paper to highlight the results with persistent
   invariants.

-----

** CONCERN #2: "The related work section is ill-organized and informal. It
   should present a coherent overview of the field, and then give details,
   rather than meandering and introducing new papers apparently at random"

++ RESPONSE: We have completely rewritten the related work. Our rewrite 
   groups prior work by topic and compares and contrasts Gibraltar's approach
   with this prior work.

   Here are the changes that we made to address specific comments:
   
   (1) CONCERN #2.1: "At line 8 of page 3, papers [17,19,26] perform error
   detection. Your work belongs in this category. By contrast, the paper [21]
   performs error correction."

   RESPONSE: We have now cited the paper by Demsky et al. (ISSTA'06) under
   the recovery category. We have also cited Jeff Perkins et al.'s SOSP paper.

   (2) CONCERN #2.2: "For this reason, I also take a bit of exception to the
   term "enforcement" in the title of your paper. The standard dictionary
   definition of "enforcement" would indicate that the system is doing
   something to enforce the properties -- that is, to ensure that they are
   maintained or to keep them true during execution. Rather, Gibraltar merely
   checks the invariants."

   RESPONSE: We no longer use the term enforcement in the paper; rather, 
   we call it rootkit detection. To address this comment, we have made
   changes throughout the paper, including to the title of the paper.

   (3) CONCERN #2.3 "In that same paragraph, the last sentence does not seem to
   be a real difference -- each of the tools observes memory snapshots, and
   Gibraltar does use the source code to aid in this. You need to remove this
   claim, or else support it."
    
   RESPONSE: We have removed this claim.

-----

** CONCERN #3: "why will the data structure extractor encounter inconsistencies? 
   Explain."

++ RESPONSE: We have explained the concept of inconsistencies better both in 
   Section 4.2, as well as in the Limitations section. We have also outlined
   possible directions for future work that could mitigate this limitation.

-----

** CONCERN #4: Invariant generator enhancements (Section 4.3). The reviewer
   had a number of serious concerns here. We addressed each of these concerns
   as discussed below.
   
   (1) CONCERN #4.1: "The paper says that the authors made 4 key changes to Daikon.
   As far as I can tell, this is not accurate. Instead, the authors have made
   no changes to the Daikon invariant detector. Rather, they have implemented a
   new front end.  Explaining the work in this fashion will make it far more
   comprehensible."

   RESPONSE: This was a terminology mistake on our part that has now been fixed
   in the paper. We refer to all the changes that we made as developing a new
   front-end for Daikon, one that is capable of analyzing memory snapshots.


   (2) CONCERN #4.2: "The paper talks at length about traces without explaining
   the term trace.  "Trace" suggests a sequence of values, but you seem to use
   it to mean one snapshot. I suggest that you dispense with the term "trace"
   and instead use the paper's own term "snapshot". One snapshot corresponds to
   one program point in Daikon, and a sequence of these would be a trace (if
   you wanted to use that term, which I would suggest you not do). Multiple
   traces would come from multiple executions of a program (which you seem not
   to do).

   RESPONSE: Following your suggestion, we have dispensed with the use of the
   term "trace" in Section 4.3. Each "Snapshot" contains values for the data
   structures that we extracted from a memory image. We converted each
   "snapshot" into the equivalent of a Daikon trace (not *program point*).
   Inference happens across multiple snapshots (equivalent to inference across
   multiple Daikon traces). Our prose in the paper reflects this.

   We did not want to confuse the reader by introducing new terminology, and
   have therefore attempted to explain concepts without introducing new terms.
   Where necessary, we simply give the closest Daikon front-end (e.g., kvasir)
   analogues to explain concepts.


   (3) CONCERN #4.3: "I found section 4.3 inconsistent and hard to follow. Like
   section 2, it has no clear organization, nor helpful topic sentences. It
   needs to be rewritten"

   RESPONSE: We have completely rewritten Section 4.3, with topic sentences
   and, in our opinion, a much better organization than before.


   (4) CONCERN #4.4: "Objects don't have names -- or, rather, they may have
   many names. You really mean the name of a variable or expression that
   references the object"

   RESPONSE: To address this concern, we have a new discussion on naming
   schemes used by Gibraltar -- See section 4.2.2. 

   (5) CONCERN #4.5: "How is #3 (Converting snapshots to traces) different
   than #1 (Inference over snapshots)? The paper would read better if these two
   sections were merged.  Conceptually, you can imagine a single "program
   point" that holds every variable in the entire kernel. Would that give the
   same results as your approach did? Would it be clearer to explain the system
   in those terms?  Splitting up one humongous program point into multiple
   small ones is then a performance optimization, but not a core part of the
   algorithm, and it can be relegated to elsewhere. The current writing that
   mixes this optimization into the key algorithm serves to confuse both
   issues."

   RESPONSE: As in CONCERN #4.2, we felt that the use of kvasir front-end
   terminology to explain concepts in our front-end resulted in the above
   confusion. Our revisions in Section 4.3 dispenses with kvasir front-end
   terminology, and simply explains concepts in terms of variable declarations
   variable values, and invariant inference over different observations of
   variable values.


   (6) CONCERN #4.6: "Page 9, column 2, line 46: it's true that Daikon does not
   infer invariants over lists, only arrays. Your approach is the same as that
   of other front ends. You should cite the existing Daikon front ends that
   also convert lists to arrays.  The current writing might give the mistaken
   impression that this is your contribution.

   RESPONSE: We have cited work by Ernst et al on linearizing collections.

   (7) CONCERN #4.7: "Page 10, line 49 should start a new section 4.3.2 since
   it segues to a very different topic than the first half of section 4.3"
   
   RESPONSE: This now starts as a new paragraph with a topic phrase to
   signify that we have segued to a different topic.

   (8) Page 11, line 20: "is 178940 the number of invariants that Daikon tried,
   or the number that it reported as true? Is this the result of 1 run of
   Gibraltar, or many? Numbers without context are not very helpful."

   RESPONSE: We've added a clarification.

-----

** CONCERN #5: Experimental methodology

   (1) CONCERN #5.1 The use of one micro-benchmark is questionable. Even if you
   include Lmbench, why not include a real workload, or at least a real
   program, too?  Readers can have no intuition regarding whether your results
   will translate to a more realistic setting. I was surprised that the journal
   version did not extend the conference paper in this way.

   RESPONSE: Note that we did use several workloads. LMBench was only used for
   inference. We used other workloads for detection and to measure the number
   of spurious alerts. We only used the LMbench workload for inference because
   it contains several subtasks that reflects the behavior of real users.

   (2) CONCERN #5.2: "In section 5.1, "Gibraltar run in enforcement mode": What
   was the Linux system doing? Completely identical tasks to before? Different
   ones? This makes a huge difference to whether your results are awesome or
   ho-hum, and must be made clear. Likewise, what was the relationship of the
   benign workload to the training workload?"

   RESPONSE: We have added a new subsection (now numbered Section 5.1) in which
   we explain the different workloads used in our system. The workload used
   during inference was completely different from the workload used during
   "enforcement" (now called detection)

   (3) CONCERN #5.3: "In section 5.3, it is rather surprising that *every*
   rootkit violated an invariant of the form var == constant. I think this is
   worth highlighting.  Is this a consequence of your workloads, your rootkits,
   or rootkits in general?  Can Daikon be restricted to such invariants, which
   would make it more scalable?  Discuss."

   RESPONSE: Please note that invariants of the form var == constant were only
   violated by all rootkits that affected function pointers. More sophisticated
   rootkits violated other kinds of invariants. Nevertheless, we have included
   a discussion on this issue, as requested.

   (4) CONCERN #5.4: Given that persistent invariants were (1) sufficient for
   your purposes and (2) had much lower false positive rates, I am surprised
   that the paper discusses transient invariants so thoroughly? I suggest
   rewriting the paper to use only persistent invariants throughout. Then, you
   can add a single paragraph in which you say you also tried non-persistent
   ones, and present the (worse) results.  Eliminating the distinction between
   persistent and transient invariants in the main body of the paper will
   streamline and clarify it, as well."

   RESPONSE: We have addressed this concern by refocusing the paper to talk
   about persistent invariants. We have also clarified the number of spurious
   alerts observed with persistent invariants and when all invariants are
   included. However, we were loathe to eliminate the discussion of transient
   invariants completely. This is because transient invariants also represent
   properties of objects that hold for a specific boot of the system (they do
   not survive reboots). We feel that transient invariants are an important
   part of Gibraltar, and that future enhancements to Gibraltar can reduce the
   number of spurious transient invariants. We therefore wanted to keep the
   distinction in the paper.

   (5) CONCERN #5.5: "Also, on page 13 line 50, I don't understand how some
   data structures can have no portable names. Every data structure is
   reachable from some static variable, and there is some portable name for the
   path to the data structure. Please explain better what you mean.

   RESPONSE: The problem is that the path to a specific object may differ
   across reboots of the machine. This is best illustrated by Figure 15 in 
   the paper and the associated explanation - the name of this object depends
   upon its location in the linked list.

   (6) CONCERN #5.6: "In section 5.5, I am very impressed that you got such
   good results from just 15 snapshots and total training time of less than 1
   hour. Some papers in the literature claims that very extensive training
   suites are required (and other papers claim the opposite). Your result is a
   valuable contribution and is worth highlighting as such. Also, please try a
   larger run and report the results."

   RESPONSE: We have tried a larger run (30 snapshots) and observed that fewer
   than 0.01% of the invariants changed when we used a larger number of
   snapshots. We have reported this fact in the paper.

   (7) CONCERN #5.7: "I'm confused by section 5.5, part (1). If you give Daikon
   all the snapshots, then it does the incremental refinement that you propose.
   Please describe what Daikon's algorithm is, if it is not already doing
   exactly the refinement you propose."

   RESPONSE: We have clarified what we meant above.

   (8) CONCERN #5.8: "Page 13, top of column 2: The claim that static analysis
   can be sound and complete is just silly. Retract it. Static analysis can be
   sound, but only at the price of incompleteness."

   RESPONSE: Done. We instead reference recent work on symbolic execution 
   (e.g., KLEE) as a possible way to address this issue.


===============================================================================

Comments by Reviewer #3
-----------------------

** CONCERN #1: "For real attacks, the system administrator would not know what
   data structures will be modified.  Did you infer different specifications
   for each attack or did you have one specification that you used across all
   attacks?  Did you infer the specifications before or after you knew the
   details of the attack?  It would be helpful to detail the entire history of
   the experiments."

++ RESPONSE: Invariants were inferred once, automatically, before we knew
   any details of each attack. There were several thousand specifications.
   The same set of specifications was able to detect all 23 rootkits. Our
   experimental methodology section has now been improved to clarify these
   points.

-----

** CONCERN #2: Does the presence of PCI cards that can directly read memory
   contents pose a security threat in that if an attacker manages to compromise
   the monitoring computer they can easily obtain the information necessary to
   compromise all of the monitored computers?

++ RESPONSE: Yes, it does. However, we assume that the monitoring computer is
   part of the trusted computing base.

-----

** CONCERN #3: Although a false positive rate of 0.65% sounds low for a 42
   minute workload, it seems this means that false warnings would occur
   relatively regularly (once every few weeks per machine).  Do you have any
   evidence of what the maximum acceptable false positive rate is?

++ RESPONSE: We have not conducted a user study to predict the effects of
   this. However, this could be a topic for future research.

-----

** CONCERN #4: You might discuss memory curtaining in the limitation sections.
   In particular a root kit may be able to use mechanisms designed for
   protected computing to prevent the PCI bus from examining protected data
   structures.

++ RESPONSE: Good point - in fact, this is the basic idea behind Rutkowska's
   attack as well, which we reference in footnote #2.

===============================================================================
