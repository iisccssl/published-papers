\section{Design of \TOOL}
\label{section:design}

\tool\ aims to find bugs in Xamarin by generating apps, executing these apps on
Windows Phone and Android, and looking for inconsistencies in them. Thus,
\tool's design consists of two parts, the test case generator and the
inconsistency checker.

\myparagraph{Test Case Generation}
%
\tool\ generates test cases that exercise the programming API used by Windows
Phone developers. As illustrated in \figref{section:example}, each test case is
a sequence of method calls to this API. The arguments to these calls are either
values with primitive data types, or references to objects constructed and
modified by method calls appearing earlier in the sequence. The main challenge
is to generate meaningful method sequences that are also effective, \ie~the
test case generator should be able to elicit error cases in Xamarin without
generating too many test cases.

This problem has been investigated in the past in the context of generating
unit tests for object-oriented programs, and tools such as
JCrasher~\cite{jcrasher}
and Randoop~\cite{randoop:icse07} implement such test case generation. In
particular, Randoop uses a \textit{feedback-directed approach} to random test
generation and is the basis for \tool's test generator as well. We now briefly
describe Randoop's (and therefore \tool's) approach to test generation.

The test generator accepts as input a list of classes to be tested, a set of
filters and contracts (which are sanity checks to be performed), and a timeout.
Intuitively, the test generation algorithm iteratively ``grows'' method
sequences from previously-generated shorter sequences. Suppose that the test
generator has already generated a set of method sequences as valid test cases.
During each iteration, the test generator picks a method \code{m(T$_1$,
$\ldots$, T$_n$)} at random from the list of classes provided to it as input,
and ``extends'' the existing method sequences with a call to \code{m} (\eg~one
way to ``extend'' is to append \code{m} to the end of the sequence). If the
parameters of \code{m} are all of primitive type, then the test generator
randomly selects the values of these parameters from a pool of acceptable
values. If the parameter is a reference to an object, then the test generator
uses an object of suitable type created by a method in the sequence that
\code{m} just extended (or passes a \textsc{null} reference). \tool\ then wraps
this method sequence with template code to serialize data structures and to
catch exceptions, as shown in the examples from \sectref{section:example}, to
produce the test cases.

The test generator then executes the newly-generated test sequences looking for
violations of filters and contracts. These are sanity checks that look for
common error cases, such as test cases that throw an exception, or those that
violate certain invariants (\eg~\code{o.equals(o)} not returning \textsc{true}).
Test sequences that violate these sanity checks are discarded, and the
remaining test cases are added to the set of valid test cases, to be extended
in future iterations.  This process continues till the specified timeout has
expired.  This iterative approach is key to generating effective test cases. It
ensures that every prefix of a valid test sequence is also valid, and that test
sequences that violate simple sanity conditions (\eg~those that throw an
exception) are never extended.

\myparagraph{Serializing State and Checking Inconsistencies}
%
For the test cases generated using the approach above, \tool\ produces a pair
of apps for Windows Phone and Android. It executes them atop these platforms to
observe inconsistent behavior. We now discuss the design of the serializer,
which helps identify inconsistencies when both apps return \textsc{success},
\ie~the first case in \figref{table:inconsistency-sources}. The other two cases
are straightforward and we do not discuss them further.

The serializer recursively traverses object references to create serialized
representations. Intuitively, a serialized representation is a set of
(\textsf{key}, \textsf{value}) pairs. The \textsf{key} is the name of a public
field of the object. For fields of primitive type (\eg~\code{bool}, \code{int},
\code{String}), the \textsf{value} is simply the actual value of the field. For
fields that are themselves object references, the value is a serialized
representation of that object. The example below shows the serialized
representation of a linked list with two entries. The \code{data} field of the
entries store \code{1} and \code{2}, respectively.
%
$$
\{
	(``\code{data}", \code{1}),
	(``\code{next}", \{(``\code{data}", \code{2}), 
	                   (``\code{next}", \textsc{null})
	                 \})
\}
$$

\tool's serializer uses the \code{Json.NET}~\cite{jsonnet} library, which
optionally supports the ability to serialize cyclic data structures. It does so
by keeping track of object references using an additional identifier. However,
in our experience, the random test cases that we generate do not produce cyclic
heap data structures. We therefore did not enable support for serializing
cyclic data structures in our prototype, and the serialized object
representations are tree-structured as a result. Note that in the unlikely case
that a test case does produce a cyclic data structure, our serializer would
infinitely loop---a situation we have not encountered to date in our
experiments.

\tool\ identifies inconsistencies by comparing serialized representations of
objects on the home and target platforms. Comparison proceeds recursively in a
bottom-up fashion. All the (\textsf{key}, \textsf{value}) pairs storing
primitive types must match, and the tree-structure of the serialized
representation must be the same, \ie~the same \textsf{key}s on both platforms
at each level of the tree. Any mismatches indicate inconsistent state. In most
of the bugs that we found, the mismatches were because the \textsf{value}s
diverged (\eg~the complex number example in \figref{figure:inconsist-eg}).
However, we also found cases where the fields in the objects were different on
Windows Phone and Android because a field that was declared to be \code{public}
on Windows Phone was a \code{private} field in Android, and therefore not
listed in the serialized representation. 

As previously discussed, the feedback-directed approach to test case generation
does not extend any method sequences that violate its filters and contracts,
\eg~sequences that throw an exception when executed. While Randoop was
originally designed for unit-testing object-oriented programs, \tool\ extends
it for cross-platform differential testing. For practical reasons described in
\sectref{section:implementation}, \tool\ first executes the test case generator
on one platform, where it uses the iterative approach to output test cases.  It
then executes these test cases on the target platform (Android). Thus, \tool's
test cases also preserve the property that only non-exception-generating test
cases are extended in the iterative process. 

However, because \tool\ generates all the test cases on the home platform
before executing them on the target platform, even those test cases that return
\textsc{success} but produce inconsistent serialized state across the two
platforms are extended during test generation. As a result, it is possible that
multiple test cases produced by \tool\ may report the same inconsistency.

\myparagraph{Discussion}
%
Differential testing offers an attractive property. If a test case is executed
on two API implementations with the same initial state, any inconsistency in
the final states indicates a problem in at least one of the API
implementations. That is, differential testing \textit{does not produce false
positives.}

However, in practice, it is possible that an inconsistency does not always
correspond to a problem. In our experiments, we found that such a situation
could arise because of any one of a small number of reasons.  First, some API
methods, such as those from \code{System.Random} and \code{System.Time}, invoke
platform-specific features and return different values when invoked on
different platforms. For example, the \code{System.Net.Cookie()} constructor
initializes \code{Cookie.TimeStamp} with the current system time. Unless the
emulation environments that run the apps for both the home and target platforms
are synchronized, this call will return different values.  Second, for some
methods, such as \code{Object.GetHashCode}, the documentation specifies that
the behavior of the method varies across platforms. That is, the
\code{HashCode} of an object can be different on the home and target platforms
even if the serialized representations of the object are the same on both
platforms. A third source of false positives was because the Mono runtime
included in a Xamarin-produced Android app uses Mono Assemblies as its
libraries. These libraries have different metadata information than their
Windows Phone counterparts, and any calls that access this metadata will result
in inconsistent serialized state.

\begin{figure}[t!]
\centering
\scriptsize
\begin{tabular}{|l@{~|~}p{0.25\textwidth}|}
\hline
\multicolumn{2}{|c|}
{\textbf{Filtered classes:} All methods/variables of this class filtered.}\\
\hline
\code{System.Random} & 
	Members return random values\\
\hline
\hline
\multicolumn{2}{|c|}
{\textbf{Filtered methods:} Methods cannot appear in test cases.}\\
\hline
\code{System.Type GetType()} & 
	Return value may include information from the underlying C\# assembly, which
	is not uniform across platforms\\
\code{System.Int32 GetHashCode()} & 
	Documentation specifies that hash code of similar objects need not be similar across platforms\\
\hline
\hline
\multicolumn{2}{|c|}
{\textbf{Filtered constructors:} Constructor cannot appear in test cases.}\\
\hline
\code{System.Xml.UniqueId()} & 
	Returns a unique GUID, which is not guaranteed to be consistent across
	platforms\\
\hline
\hline
\multicolumn{2}{|c|}
{\textbf{Filtered fields/properties:} Cannot be accessed in test cases.}\\
\hline
\code{System.Net.Cookie.TimeStamp} & Value relies on system time at object creation\\
\code{System.DateTimeOffset.Now}   & Value relies on system time\\
\code{System.DateTimeOffset.UtcNow} & Value relies on system time\\
\code{System.DateTime.Now}         & Value relies on system time\\
\code{System.DateTime.UtcNow}      & Value relies on system time\\
\code{System.DateTime.Today}       & Value relies on system time\\
\code{System.Exception.HResult}    & Value identifies an exception, but
documentation is not conclusive about whether value is consistent across
platforms\\
\hline
\end{tabular}
\mycaption{Filters used by \tool\ to avoid generating test cases that produce false positives.}
{\label{figure:filters}}
\end{figure}

Fortunately, it is relatively easy to filter out test cases that can
potentially lead to such false positives. We simply integrate filters that
prevent the test generator from producing method sequences that contain method
calls or field references that can potentially trigger such false positives.
Thus, with just a few filters to eliminate the causes above (see
\figref{figure:filters}), we were able to eliminate false positives, thereby
ensuring that all inconsistencies reported by \tool\ indeed correspond to real
bugs.  Note, however, that as with most other testing tools, our differential
testing approach does have false negatives---\ie~it is not guaranteed to find
all possible inconsistencies.
